/*
 * Copyright Â© 2019 Cask Data, Inc.
 *
 * Licensed under the Apache License, Version 2.0 (the "License"); you may not
 * use this file except in compliance with the License. You may obtain a copy of
 * the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 * License for the specific language governing permissions and limitations under
 * the License.
 */
package io.cdap.plugin.gcp.datastore.sink;

import com.google.api.client.util.BackOff;
import com.google.api.client.util.ExponentialBackOff;
import com.google.api.client.util.Sleeper;
import com.google.common.collect.MoreCollectors;
import com.google.datastore.v1.AllocateIdsRequest;
import com.google.datastore.v1.AllocateIdsResponse;
import com.google.datastore.v1.BeginTransactionRequest;
import com.google.datastore.v1.BeginTransactionResponse;
import com.google.datastore.v1.CommitRequest;
import com.google.datastore.v1.Entity;
import com.google.datastore.v1.client.Datastore;
import com.google.datastore.v1.client.DatastoreException;
import com.google.datastore.v1.client.DatastoreHelper;
import com.google.protobuf.ByteString;
import com.google.rpc.Code;
import io.cdap.plugin.gcp.datastore.sink.util.DatastoreSinkConstants;
import io.cdap.plugin.gcp.datastore.util.DatastoreUtil;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.mapreduce.Counter;
import org.apache.hadoop.mapreduce.RecordWriter;
import org.apache.hadoop.mapreduce.TaskAttemptContext;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormatCounter;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.io.IOException;

/**
 * {@link DatastoreRecordWriter} writes the job outputs to the Datastore. Accepts <code>null</code> key, FullEntity
 * pairs but writes only FullEntities to the Datastore.
 */
public class DatastoreRecordWriter extends RecordWriter<NullWritable, Entity> {

  private static final Logger LOG = LoggerFactory.getLogger(DatastoreRecordWriter.class);

  private final Datastore datastore;
  private final int batchSize;
  private final boolean useAutogeneratedKey;
  private final boolean useTransactions;
  private CommitRequest.Builder builder;
  private int totalCount;
  private int numberOfRecordsInBatch;
  private String projectId;
  private Counter counter;
  private Sleeper sleeper;
  private BackOff flushBackoff;

  public DatastoreRecordWriter(TaskAttemptContext taskAttemptContext) throws IOException {
    Configuration config = taskAttemptContext.getConfiguration();
    this.projectId = config.get(DatastoreSinkConstants.CONFIG_PROJECT);
    String serviceAccount = config.get(DatastoreSinkConstants.CONFIG_SERVICE_ACCOUNT);
    Boolean isServiceAccountFilePath = config.getBoolean(DatastoreSinkConstants.CONFIG_SERVICE_ACCOUNT_IS_FILE_PATH,
                                                         true);
    this.batchSize = config.getInt(DatastoreSinkConstants.CONFIG_BATCH_SIZE, 25);
    this.useAutogeneratedKey = config.getBoolean(DatastoreSinkConstants.CONFIG_USE_AUTOGENERATED_KEY, false);
    this.useTransactions = config.getBoolean(DatastoreSinkConstants.CONFIG_USE_TRANSACTIONS, true);
    LOG.debug("Initialize RecordWriter(projectId={}, batchSize={}, useAutogeneratedKey={}, "
      + "serviceAccount={})", projectId, batchSize, useAutogeneratedKey, serviceAccount);

    this.datastore = DatastoreUtil.getDatastoreV1(serviceAccount,
                                                  isServiceAccountFilePath,
                                                  projectId);

    this.totalCount = 0;
    this.numberOfRecordsInBatch = 0;
    this.builder = newCommitRequest();
    this.counter = taskAttemptContext.getCounter(FileOutputFormatCounter.BYTES_WRITTEN);
    this.sleeper = Sleeper.DEFAULT;
    this.flushBackoff = new ExponentialBackOff.Builder()
      .setMaxIntervalMillis(DatastoreSinkConstants.FLUSH_MAX_BACKOFF_MILLIS)
      .setInitialIntervalMillis(DatastoreSinkConstants.FLUSH_INITIAL_BACKOFF_MILLIS)
      .setMaxElapsedTimeMillis(DatastoreSinkConstants.FLUSH_MAX_ELAPSED_TIME)
      .setRandomizationFactor(DatastoreSinkConstants.FLUSH_RANDOMIZATION_FACTOR)
      .build();
  }

  private CommitRequest.Builder newCommitRequest() throws IOException {
    // Execute the RPC synchronously.
    CommitRequest.Builder builder = CommitRequest.newBuilder();
    builder.setProjectId(projectId);

    if (useTransactions) {
      builder.setMode(CommitRequest.Mode.TRANSACTIONAL);
      // Create an RPC request to begin a new transaction.
      BeginTransactionRequest.Builder treq = BeginTransactionRequest.newBuilder();
      BeginTransactionResponse tres;
      try {
        tres = datastore.beginTransaction(treq.build());
      } catch (DatastoreException e) {
        throw new IOException("Failed to begin datastore transaction", e);
      }
      // Get the transaction handle from the response.
      ByteString tx = tres.getTransaction();
      builder.setTransaction(tx);
    } else {
      builder.setMode(CommitRequest.Mode.NON_TRANSACTIONAL);
    }

    return builder;
  }

  @Override
  public void write(NullWritable key, Entity entity) throws IOException, InterruptedException {
    LOG.trace("RecordWriter write({})", entity);
    if (useAutogeneratedKey) {
      AllocateIdsRequest request =
        AllocateIdsRequest.newBuilder()
          .setProjectId(projectId)
          .addKeys(entity.getKey())
          .build();
      AllocateIdsResponse response;
      try {
         response = datastore.allocateIds(request);
      } catch (DatastoreException e) {
        throw new IOException("Failed to allocate id", e);
      }

      Entity fullEntity = Entity.newBuilder()
        .setKey(response.getKeysList().stream().collect(MoreCollectors.onlyElement()))
        .putAllProperties(entity.getPropertiesMap())
        .build();
      builder.addMutations(DatastoreHelper.makeInsert(fullEntity).build());
    } else {
      builder.addMutations(DatastoreHelper.makeUpsert(entity).build());
    }
    ++totalCount;
    ++numberOfRecordsInBatch;
    if (totalCount % batchSize == 0) {
      flush();
    }
  }

  @Override
  public void close(TaskAttemptContext taskAttemptContext) throws IOException, InterruptedException {
    flush();
    LOG.debug("Total number of values written to Cloud Datastore: {}", totalCount);
  }

  private void flush() throws IOException, InterruptedException {
    if (numberOfRecordsInBatch > 0) {
      LOG.debug("Writing a batch of {} values to Cloud Datastore.", numberOfRecordsInBatch);

      while (true) {
        try {
          flushInternal();
          break;
        } catch (DatastoreException e) {
          long backoff = flushBackoff.nextBackOffMillis();

          // If the exception is retryable and we haven't exceeded our deadline, try again.
          if (backoff != BackOff.STOP && isRetryable(e)) {
            LOG.warn("Retrying flush after {} ms", backoff);
            sleeper.sleep(backoff);
            continue;
          }

          LOG.error("Datastore commit failed with code {}: {}", e.getCode(), e.toString());
          throw new IOException("Datastore commit failed", e);
        }
      }

      builder = newCommitRequest();
      numberOfRecordsInBatch = 0;
      flushBackoff.reset();
    }
  }

  private void flushInternal() throws DatastoreException {
    CommitRequest request = builder.build();
    datastore.commit(request);
    counter.increment(request.getSerializedSize());
  }

  private boolean isRetryable(DatastoreException e) {
    return e.getCode() == Code.ABORTED || e.getCode() == Code.DEADLINE_EXCEEDED
      || e.getCode() == Code.RESOURCE_EXHAUSTED || e.getCode() == Code.UNAVAILABLE;
  }
}
